[//]: # (Image References)

[image1]: ./images/star_schema_model_songplays.png "Model"
[image2]: ./images/filepaths.png "Filepaths songdata"
[image3]: ./images/song_file_example.png "Song file example"
[image4]: ./images/filepaths_logdata.png "Filepaths logdata"
[image5]: ./images/log_file_example.png "Log file example"
[image6]: ./images/notebook.png "Notebook"
[image7]: ./images/run_notebook.png "Run notebook"
[image8]: ./images/star_schema_model_songplays.png "Star Schema"
[image9]: ./images/result_1.png "Result"
[image10]: ./images/result_null.png "Result null"
[image11]: ./images/result_count.png "Result count"


# Project: Data Modeling with PostgreSQL
This project is part of the [Data Engineering Nanodegree Program](https://www.udacity.com/course/data-engineer-nanodegree--nd027), by [Udacity](https://www.udacity.com/).  

## Introduction  

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

They'd like a data engineer to create a Postgres database with tables designed to optimize queries on song play analysis, and bring you on the project. Your role is to create a database schema and ETL pipeline for this analysis. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.  

### Project Description  

In this project, I applied what I've learned on data modeling with Postgres and build an ETL pipeline using Python. To complete the project, I needed to define fact and dimension tables for a star schema for a particular analytic focus, and wrote an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL.

# Star Schema for Song Play Analysis
For this project, I used the song and log datasets and created a [star-schema model](https://en.wikipedia.org/wiki/Star_schema) optimized for queries on song play analysis. 

Star and snowflake schemas use the concepts of **fact** and **dimension** tables to make getting value out of the data easier.  

- **Fact table** consists of measurements, metrics or facts a business process.
- **Dimension table** a structure that categorizes facts and measures in order to enable users to answer business questions.

**Benefits of star-schema:**  

- Star-schemas are denormalized. Denormalization increases performance by reducing the number of joins between tables. Data integrity will take a bit of a potential hit, as there will be more copies of the data (to reduce JOINS). The benefits of star-schema denormalization are:  

	- Simpler queries
	- Simplified business reporting logic
	- Query performance gains
	- Fast aggregations
	- Feed cubes
	- Fast aggregations
	
         
## Data modeling

For this project, I modeled a star-schema that includes one fact table and four dimension tables. See the descriptions below:

**Fact Table**  

- **songplays:** records in log data associated with song plays. The log data was filtered with page = 'NextSong'.  

	- Columns: songplay\_id, start\_time, user\_id, level, song\_id, artist\_id, session\_id, location, user\_agent.  

**Dimension Tables**  

- **users:** users in the app.  
	- Columns: user\_id, first\_name, last\_name, gender, level.

- **songs:** songs in music database.  
	- Columns: song\_id, title, artist\_id, year, duration  

- **artists:** artists in music database.
	- Columns: artist\_id, name, location, latitude, longitude  

- **time:** timestamps of records in songplays broken down into specific units.
	- Columns: start\_time, hour, day, week, month, year, weekday  
	
![Data Modeling][image8]  
	
# Datasets  

## Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

![Filepaths][image2]

And below is an example of what a single song file, TRAAAAW128F429D538.json, looks like.

![Song file example][image3]  

## Log Dataset  
 
The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

![Filepaths logdata][image4]

And below is an example of what the data in a log file, 2018-11-01-events.json, looks like.  

![Log file example][image5]


# ETL pipeline

I created three files **.py** that can be run by Anaconda Prompt (Windows) or Terminal (Linux) without the help of a graphic interface. The files are organized as follows:  

- **sql_queries.py:** 
	- contains sql queries to: drop tables, create tables, and insert tables. There are two lists:  
		- **create\_table\_queries:** for each 'create table' command was assigned a variable name that is listed here.
		- **drop\_table\_queries:** for each 'drop table' command was assigned a variable name that is listed here.
	- for each 'insert table' command was assigned a variable name
	- contains the **song\_select query** to find the song\_id and artist\_id based on the title, artist\_name, and duration of a song.  

The lists are imported and used into create\_tables.py and the select query is imported and used into etl.py.  

- **create_tables.py:**  
	- creates a database for this project
	- imports the psycopg2 to create a connection and a cursor for this project
	- imports the lists from sql\_queries.py 
	- defines a function that uses the drop\_table\_queries list and runs a drop commands sequentially
	- defines a function that uses the create\_table\_queries list and runs a create commands sequentially  

- **etl.py:**  
	- imports the sql\_queries.py 
	- defines functions that:
		- read the data files (log\_data and song\_data)
		- create data frame for data files
		- insert data into each table. The functions use the variable names defined for each 'insert table' command in **sql_queries.py**  
	- runs the **song\_select query** to insert data into songplays
	
### Results after runnings

Since this is a subset of the much larger dataset, the solution dataset will only have 1 row with values for value containing ID for both song\_id and artist\_id in the fact table. The rest of the rows will have NONE values for those two variables. Below I display the result:  

- One row with values for value containing ID for both song\_id and artist\_id in the fact table  

![Result][image9]

- The rest of the rows will have NONE values  

![Result Null][image10]  

- Total of rows will have NONE values 

![Result Count][image11]

# Getting Started
## Files included in this repository  
The project includes six files:  

 1. **test.ipynb:** displays the first few rows of each table to let you check your database.  
 2. **create_tables.py:** drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.  
 3. **etl.ipynb:** reads and processes a single file from song_data and log_data and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.  
 4. **etl.py:** reads and processes files from song_data and log_data and loads them into your tables. You can fill this out based on your work in the ETL notebook.  
 5. **sql_queries.py:** contains all your sql queries, and is imported into the last three files above.  
 6. **README.md:** provides discussion on your project. 

## Preparing the environment to run the project
**1.** Install the Anaconda Plataform

- For more information about it, see [Anaconda Installation Guide](https://docs.anaconda.com/anaconda/install/) and [User guide](https://docs.anaconda.com/anaconda/user-guide/)

- Download the [Anaconda Installer](https://www.anaconda.com/distribution/), version Python 3.x  
conda  

**2.** Create and activate a new environment for this project

- Open Anaconda Prompt (Windows) or Terminal (Linux) and type

	```
	conda create --name datamodeling
	```  

	```
	conda activate datamodeling
	```
	
**3.** Required libraries for this project

Some libraries will be installed with Anaconda installer and others need to be installed later. For this project you will need:  

- ipython-sql (not provided by Anaconda installer)  
- jsonschema  
- jupyter notebook  
- numpy  
- pandas  
- psycopg2 (not provided by Anaconda installer)  
- python  	

**4.** In your new environment check if the required libraries were installed by Anaconda installer  

- In your Anaconda Prompt (Windows) or Terminal (Linux) type the line command below and verify the packages installed by Anaconda.
		
	```
	conda list
	```  

- Install libraries not provided by Anaconda installer  
	- In your Anaconda Prompt (Windows) or Terminal (Linux) type the line command below and install the required libraries. Below is an example:
	
	```
	conda install ipython-sql 
	```  

**5.** Install the PostgreSQL. For more information see [postgresql.org](https://www.postgresql.org/)  

PostgreSQL is a powerful, open source object-relational database system with over 30 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance.

## Getting the code 
There are two options to get this project:

- Download it as a zip file

	- Do the download in this [link](https://github.com/LucianaRocha/data-modeling-with-postgresql/archive/master.zip).
	- Unzip the file in a folder of your choice.  

- Clone this repository using Git version control system

	- Open Anaconda Prompt (Windows) or Terminal (Linux), navigate to the folder of your choice, and type the command below:  

	```
	git clone https://github.com/LucianaRocha/data-modeling-with-postgresql.git
	```


# Running the project  

There are two options to run this project:  

1. By Jupyter Nobebook, or
2. By Anaconda Prompt (Windows) or Terminal (Linux)

## By Jupyter Nobebook  

-  Open Anaconda Prompt (Windows) or Terminal (Linux), navigate to the project folder, and type the commands below to activate the project environment, and to open Jupyter.
If you are keen to know more about notebooks and other tools of Project Jupyter, you find more information on this [website](https://jupyter.org/index.html).

	```
	conda activate datamodeling
	```
	
	```
	jupyter notebook
	```	

- Click on the etl.ipynb to open the notebook and follow the instructions within it.

![Notebook][image6]

- Run the notebook.  

![Run notebook][image7]  

- Use the test.ipynb to display the first few rows of each table. Alternatively you can use the Query Tool of the Postgresql.

## By Anaconda Prompt (Windows) or Terminal (Linux)

- Open Anaconda Prompt (Windows) or Terminal (Linux), navigate to the project folder, and type the command below to create the tables:

	```
	python create_tables.py
	```

- In your terminal, after creating all tables of the previous step, type the command below to load the tables:  
		
	```
	python etl.py
	```	
- Use the test.ipynb to display the first few rows of each table. Alternatively you can use the Query Tool of the Postgresql.

# Ideas for future work
There are many BI techniques and I would like improve this model with [Kimball Techniques](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/):  

- [Slowly changing dimension](https://www.kimballgroup.com/2013/02/design-tip-152-slowly-changing-dimension-types-0-4-5-6-7/)
- [Handling Null Foreign Keys in Fact Tables](https://www.kimballgroup.com/2010/10/design-tip-128-selecting-default-values-for-nulls/)
- [White Paper: An Architecture for Data Quality](https://www.kimballgroup.com/2007/10/white-paper-an-architecture-for-data-quality/)

# References
- Data Warehouse and Business Intelligence Resources [Kimball Techniques](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/)
- Pandas API reference [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/index.html)
- PEP8 [style guidelines](https://www.python.org/dev/peps/pep-0008/)
- PostgreSQL [documentation](https://www.postgresql.org/docs/11/index.html)
- psycopg [documentation](http://initd.org/psycopg/docs/)
- Python Database API [summary](https://wiki.python.org/moin/DbApiCheatSheet)
- Python errors and exceptions [documentation](https://docs.python.org/3/tutorial/errors.html)
- SQL [Kickstarter SQL Style Guide](https://gist.github.com/fredbenenson/7bb92718e19138c20591)  
